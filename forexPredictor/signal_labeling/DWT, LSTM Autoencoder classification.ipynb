{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd8e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input, Flatten, Reshape\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1036e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File found: D:\\Repos_git\\Make_Money_with_Tensorflow_2.0\\forexPredictor\\ohlc_data\\0_61938.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20997</td>\n",
       "      <td>1.21089</td>\n",
       "      <td>1.20966</td>\n",
       "      <td>1.20999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.20481</td>\n",
       "      <td>1.20569</td>\n",
       "      <td>1.20479</td>\n",
       "      <td>1.20538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.20537</td>\n",
       "      <td>1.20574</td>\n",
       "      <td>1.20341</td>\n",
       "      <td>1.20553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.20556</td>\n",
       "      <td>1.20689</td>\n",
       "      <td>1.20442</td>\n",
       "      <td>1.20469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.20468</td>\n",
       "      <td>1.20599</td>\n",
       "      <td>1.20380</td>\n",
       "      <td>1.20573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Open     High      Low    Close\n",
       "0  1.20997  1.21089  1.20966  1.20999\n",
       "1  1.20481  1.20569  1.20479  1.20538\n",
       "2  1.20537  1.20574  1.20341  1.20553\n",
       "3  1.20556  1.20689  1.20442  1.20469\n",
       "4  1.20468  1.20599  1.20380  1.20573"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_num = 5\n",
    "sequence_length = 8  # Number of time steps to consider\n",
    "\n",
    "# Define file and directory names\n",
    "file_name = '0_61938.csv'\n",
    "data_dir = 'ohlc_data'\n",
    "# parent_dir = 'forexPredictor'\n",
    "# repo = 'Repos_git'\n",
    "# repo_dir = 'Make_Money_with_Tensorflow_2.0'\n",
    "# Get the absolute base directory dynamically\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Move up one level\n",
    "\n",
    "# Construct the full file path in an OS-independent way\n",
    "# data_path = os.path.join(base_dir, repo, repo_dir, parent_dir, data_dir, file_name)\n",
    "data_path = os.path.join(base_dir, data_dir, file_name)\n",
    "\n",
    "# Check if the file exists before using it\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"✅ File found: {data_path}\")\n",
    "else:\n",
    "    print(f\"❌ Error: File not found at {data_path}\")\n",
    "\n",
    "\n",
    "ucols=['Open', 'High', 'Low', 'Close']\n",
    "data = pd.read_csv(data_path, usecols=ucols)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f79ec846",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data\n",
    "# Convert DataFrame to NumPy array\n",
    "data = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5959bdf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61938, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0221be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(df)\n",
    "\n",
    "# Reshape for LSTM input\n",
    "data = data.reshape((data.shape[0], data.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb6ffef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61938, 4, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0921372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dwt(data):\n",
    "    coeffs = [pywt.dwt(sample, 'haar') for sample in data]\n",
    "    cA, cD = zip(*coeffs)  # Approximation and Detail coefficients\n",
    "    return np.array(cA)  # Using only approximation coefficients\n",
    "\n",
    "# Define Autoencoder with LSTM for better feature extraction\n",
    "def create_autoencoder(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = LSTM(64, return_sequences=True)(input_layer)\n",
    "    x = LSTM(32, return_sequences=False)(x)\n",
    "    encoded = Dense(16, activation='relu')(x)\n",
    "\n",
    "    decoded = Dense(32, activation='relu')(encoded)\n",
    "    decoded = Dense(input_shape[0], activation='sigmoid')(decoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder, encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11dfb53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1936/1936 [==============================] - 8s 3ms/step - loss: 0.0088\n",
      "Epoch 2/5\n",
      "1936/1936 [==============================] - 6s 3ms/step - loss: 0.0059\n",
      "Epoch 3/5\n",
      "1936/1936 [==============================] - 6s 3ms/step - loss: 0.0059\n",
      "Epoch 4/5\n",
      "1936/1936 [==============================] - 6s 3ms/step - loss: 0.0059\n",
      "Epoch 5/5\n",
      "1936/1936 [==============================] - 6s 3ms/step - loss: 0.0059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f2327fdc00>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply DWT\n",
    "transformed_data = apply_dwt(data)\n",
    "\n",
    "# Train Autoencoder\n",
    "autoencoder, encoder = create_autoencoder(transformed_data.shape[1:])\n",
    "autoencoder.fit(transformed_data, transformed_data, epochs=5, batch_size=32, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32b37df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "autoencoder.save(\"encoder_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d4647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1936/1936 [==============================] - 3s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Extract Features\n",
    "features = encoder.predict(transformed_data)\n",
    "\n",
    "# Apply TimeSeriesKMeans Clustering\n",
    "num_clusters = 3  # Buy (2), Sell (1), Hold (0)\n",
    "tskmeans = TimeSeriesKMeans(n_clusters=num_clusters, metric=\"dtw\", random_state=42)\n",
    "clusters = tskmeans.fit_predict(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d4e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert clusters to one-hot encoding\n",
    "clusters_one_hot = tf.keras.utils.to_categorical(clusters, num_clusters)\n",
    "\n",
    "# Split data into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, clusters_one_hot, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f6c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure input shape consistency\n",
    "input_shape = features.shape[1:]  # Extract feature dimension\n",
    "classifier_input = Input(shape=input_shape)\n",
    "classifier_output = Dense(num_clusters, activation=\"softmax\")(classifier_input)\n",
    "classifier_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79869e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile classifier\n",
    "final_model = Model(classifier_input, classifier_output)\n",
    "final_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "epoch_num = 5  # Define the number of epochs\n",
    "final_model.fit(X_train, y_train, epochs=epoch_num, batch_size=5, validation_data=(X_test, y_test))\n",
    "\n",
    "final_model.save(\"trading_model_dwt_autoEnc.h5\")\n",
    "\n",
    "# Evaluate model on test data\n",
    "loss, accuracy = final_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Confusion Matrix & Classification Report\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59b26b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model(\"trading_model_dwt_autoEnc.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2243a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentation: https://www.mql5.com/en/docs/integration/python_metatrader5\n",
    "\n",
    "import MetaTrader5 as mt  # pip install MetaTrader5\n",
    "import pandas as pd  # pip install pandas\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# start the platform with initialize()\n",
    "mt.initialize()\n",
    "\n",
    "# login to Trade Account with login()\n",
    "# make sure that trade server is enabled in MT5 client terminal\n",
    "\n",
    "login = 165905041\n",
    "password = 'iIeElL0176_'\n",
    "server = 'XMGlobal-MT5 2'\n",
    "\n",
    "mt.login(login, password, server)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc8ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define symbol and timeframe\n",
    "symbol = \"EURUSD\"\n",
    "timeframe = mt.TIMEFRAME_H1\n",
    "\n",
    "# Get data for the last 48 hours\n",
    "end_time = datetime.now()\n",
    "start_time = end_time - timedelta(hours=1000)\n",
    "\n",
    "# Retrieve OHLC data\n",
    "ohlc_data = pd.DataFrame(mt.copy_rates_range(symbol, timeframe, start_time, end_time))\n",
    "\n",
    "# Convert time column to datetime format\n",
    "ohlc_data['time'] = pd.to_datetime(ohlc_data['time'], unit='s')\n",
    "\n",
    "\n",
    "ohlc_data.rename(columns={'open':'Open', 'high':'High', \n",
    "                        'low':'Low', 'close':'Close', 'tick_volume':'Volume'}, inplace=True)\n",
    "columns_to_drop = ['time', 'spread', 'real_volume', 'Volume']\n",
    "ohlc_data.drop(columns=[col for col in columns_to_drop if col in ohlc_data.columns], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d96cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe5c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = ohlc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098d807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved scaler (Assuming it's saved from training to ensure consistency)\n",
    "# import joblib\n",
    "# scaler = joblib.load(\"scaler.pkl\")\n",
    "\n",
    "# Load new data\n",
    "new_data = new_data[['Open', 'High', 'Low', 'Close']].values  # Extract OHLC values\n",
    "\n",
    "# Normalize using the same MinMaxScaler used during training\n",
    "ohlc_data_scaled = scaler.transform(new_data)  # Use transform, NOT fit_transform\n",
    "\n",
    "# Reshape for LSTM input\n",
    "data = ohlc_data_scaled.reshape((new_data.shape[0], new_data.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3788dc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(713, 4, 1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dcfef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DWT\n",
    "transformed_data = apply_dwt(data)\n",
    "print(\"Transformed Data Shape:\", transformed_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b8448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained encoder and classifier\n",
    "encoder = tf.keras.models.load_model(\"encoder_model.h5\")\n",
    "classifier = tf.keras.models.load_model(\"trading_model_dwt_autoEnc.h5\")\n",
    "\n",
    "# Extract features using the trained encoder\n",
    "features = encoder.predict(transformed_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = classifier.predict(features)\n",
    "predicted_classes = np.argmax(predictions, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "# Map predictions to labels\n",
    "labels = {0: \"Hold\", 1: \"Sell\", 2: \"Buy\"}\n",
    "predicted_labels = [labels[p] for p in predicted_classes]\n",
    "\n",
    "# Align predictions with the original DataFrame\n",
    "new_data_df = pd.DataFrame(new_data, columns=['Open', 'High', 'Low', 'Close'])\n",
    "new_data_df[\"Prediction\"] = predicted_labels  # Add predicted actions to DataFrame\n",
    "\n",
    "# Save the results\n",
    "new_data_df.to_csv(\"predicted_results.csv\", index=False)\n",
    "\n",
    "# Print first 10 rows with predictions\n",
    "print(new_data_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56105e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Data\n",
    "data_path = \"stock_data.csv\"  # Update with your file path\n",
    "df = pd.read_csv(data_path, usecols=['Open', 'High', 'Low', 'Close'])\n",
    "\n",
    "# Normalize Data\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "def apply_dwt(data):\n",
    "    coeffs = [pywt.dwt(sample, 'haar') for sample in data]\n",
    "    cA, cD = zip(*coeffs)\n",
    "    return np.array(cA)  # Using only approximation coefficients\n",
    "\n",
    "# Apply DWT\n",
    "transformed_data = apply_dwt(df_scaled)\n",
    "transformed_data = transformed_data.reshape((transformed_data.shape[0], transformed_data.shape[1], 1))\n",
    "\n",
    "# Define LSTM Autoencoder\n",
    "def create_autoencoder(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = LSTM(64, return_sequences=True)(input_layer)\n",
    "    x = LSTM(32, return_sequences=False)(x)\n",
    "    encoded = Dense(16, activation='relu')(x)\n",
    "    \n",
    "    decoded = Dense(32, activation='relu')(encoded)\n",
    "    decoded = Dense(input_shape[0], activation='sigmoid')(decoded)\n",
    "    \n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    \n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Train Autoencoder\n",
    "autoencoder, encoder = create_autoencoder(transformed_data.shape[1:])\n",
    "autoencoder.fit(transformed_data, transformed_data, epochs=5, batch_size=32, verbose=1)\n",
    "\n",
    "# Extract Features\n",
    "features = encoder.predict(transformed_data)\n",
    "\n",
    "# Clustering with TimeSeriesKMeans\n",
    "num_clusters = 3  # Buy (2), Sell (1), Hold (0)\n",
    "tskmeans = TimeSeriesKMeans(n_clusters=num_clusters, metric=\"dtw\", random_state=42)\n",
    "clusters = tskmeans.fit_predict(features)\n",
    "clusters_one_hot = tf.keras.utils.to_categorical(clusters, num_clusters)\n",
    "\n",
    "# Split Data for Training\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, clusters_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Classifier\n",
    "input_shape = features.shape[1:]  # Extract feature dimension\n",
    "classifier_input = Input(shape=input_shape)\n",
    "classifier_output = Dense(num_clusters, activation=\"softmax\")(classifier_input)\n",
    "\n",
    "classifier = Model(classifier_input, classifier_output)\n",
    "classifier.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train Classifier\n",
    "classifier.fit(X_train, y_train, epochs=5, batch_size=5, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate Model\n",
    "loss, accuracy = classifier.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Confusion Matrix & Classification Report\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "\n",
    "# Predict on New Data\n",
    "def predict_new_data(new_data):\n",
    "    new_data_scaled = scaler.transform(new_data)\n",
    "    transformed_new_data = apply_dwt(new_data_scaled)\n",
    "    transformed_new_data = transformed_new_data.reshape((transformed_new_data.shape[0], transformed_new_data.shape[1], 1))\n",
    "    features_new = encoder.predict(transformed_new_data)\n",
    "    predictions = classifier.predict(features_new)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    labels = {0: \"Hold\", 1: \"Sell\", 2: \"Buy\"}\n",
    "    return [labels[p] for p in predicted_classes]\n",
    "\n",
    "# Example Usage\n",
    "new_data = pd.read_csv(\"new_stock_data.csv\", usecols=['Open', 'High', 'Low', 'Close']).values\n",
    "predictions = predict_new_data(new_data)\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
