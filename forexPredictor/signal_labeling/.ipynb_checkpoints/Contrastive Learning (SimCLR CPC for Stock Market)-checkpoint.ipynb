{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-16fe09fadd4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 5\n",
    "sequence_length = 8  # Number of time steps to consider\n",
    "\n",
    "# Define file and directory names\n",
    "file_name = '0_61938.csv'\n",
    "data_dir = 'ohlc_data'\n",
    "# parent_dir = 'forexPredictor'\n",
    "# repo = 'Repos_git'\n",
    "# repo_dir = 'Make_Money_with_Tensorflow_2.0'\n",
    "# Get the absolute base directory dynamically\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Move up one level\n",
    "\n",
    "# Construct the full file path in an OS-independent way\n",
    "# data_path = os.path.join(base_dir, repo, repo_dir, parent_dir, data_dir, file_name)\n",
    "data_path = os.path.join(base_dir, data_dir, file_name)\n",
    "\n",
    "# Check if the file exists before using it\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"✅ File found: {data_path}\")\n",
    "else:\n",
    "    print(f\"❌ Error: File not found at {data_path}\")\n",
    "\n",
    "\n",
    "ucols=['Open', 'High', 'Low', 'Close']\n",
    "data_main = pd.read_csv(data_path, usecols=ucols)\n",
    "data_main.reset_index(drop=True, inplace=True)\n",
    "data_main.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load your OHLC time series dataset\n",
    "df = pd.read_csv(\"ohlc_data.csv\", parse_dates=['Date'], index_col='Date')\n",
    "df = df[['Open', 'High', 'Low', 'Close']]  # Keep only OHLC\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "window_size = 20\n",
    "\n",
    "# 2. Create sequences for time series\n",
    "def create_sequences(data, window):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - window):\n",
    "        sequences.append(data[i:i + window])\n",
    "    return np.array(sequences)\n",
    "\n",
    "sequences = create_sequences(scaled_data, window_size)\n",
    "\n",
    "# 3. Augmentation for positive/negative pairs\n",
    "def augment_series(series):\n",
    "    noise = np.random.normal(0, 0.01, series.shape)\n",
    "    jittered = series + noise\n",
    "    return jittered\n",
    "\n",
    "def generate_ssl_triplets(sequences):\n",
    "    anchors = []\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    for i in range(len(sequences)):\n",
    "        anchor = sequences[i]\n",
    "        positive = augment_series(anchor)\n",
    "        negative_idx = (i + np.random.randint(10, 100)) % len(sequences)\n",
    "        negative = sequences[negative_idx]\n",
    "        anchors.append(anchor)\n",
    "        positives.append(positive)\n",
    "        negatives.append(negative)\n",
    "    return np.array(anchors), np.array(positives), np.array(negatives)\n",
    "\n",
    "anchor, positive, negative = generate_ssl_triplets(sequences)\n",
    "\n",
    "# 4. Define the encoder model\n",
    "def create_encoder(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(128, return_sequences=True)(inputs)\n",
    "    x = LSTM(64)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    return Model(inputs, x)\n",
    "\n",
    "encoder = create_encoder((window_size, 4))\n",
    "\n",
    "# 5. Custom training loop with contrastive loss\n",
    "def compute_loss(anchor_out, positive_out, negative_out, margin=1.0):\n",
    "    pos_dist = tf.reduce_sum(tf.square(anchor_out - positive_out), axis=-1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(anchor_out - negative_out), axis=-1)\n",
    "    loss = tf.maximum(pos_dist - neg_dist + margin, 0.0)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# 6. Training\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "optimizer = Adam()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    indices = np.arange(len(anchor))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(anchor), batch_size):\n",
    "        idx = indices[i:i+batch_size]\n",
    "        a_batch = tf.convert_to_tensor(anchor[idx], dtype=tf.float32)\n",
    "        p_batch = tf.convert_to_tensor(positive[idx], dtype=tf.float32)\n",
    "        n_batch = tf.convert_to_tensor(negative[idx], dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            a_out = encoder(a_batch, training=True)\n",
    "            p_out = encoder(p_batch, training=True)\n",
    "            n_out = encoder(n_batch, training=True)\n",
    "            loss = compute_loss(a_out, p_out, n_out)\n",
    "\n",
    "        grads = tape.gradient(loss, encoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, encoder.trainable_variables))\n",
    "        total_loss += loss.numpy()\n",
    "\n",
    "    avg_loss = total_loss / (len(anchor) // batch_size)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
