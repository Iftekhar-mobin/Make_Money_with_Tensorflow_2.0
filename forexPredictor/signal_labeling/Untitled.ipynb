{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f847615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input, Flatten, Reshape\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d184cd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File found: D:\\Repos_git\\Make_Money_with_Tensorflow_2.0\\forexPredictor\\ohlc_data\\0_61938.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20997</td>\n",
       "      <td>1.21089</td>\n",
       "      <td>1.20966</td>\n",
       "      <td>1.20999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.20481</td>\n",
       "      <td>1.20569</td>\n",
       "      <td>1.20479</td>\n",
       "      <td>1.20538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.20537</td>\n",
       "      <td>1.20574</td>\n",
       "      <td>1.20341</td>\n",
       "      <td>1.20553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.20556</td>\n",
       "      <td>1.20689</td>\n",
       "      <td>1.20442</td>\n",
       "      <td>1.20469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.20468</td>\n",
       "      <td>1.20599</td>\n",
       "      <td>1.20380</td>\n",
       "      <td>1.20573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Open     High      Low    Close\n",
       "0  1.20997  1.21089  1.20966  1.20999\n",
       "1  1.20481  1.20569  1.20479  1.20538\n",
       "2  1.20537  1.20574  1.20341  1.20553\n",
       "3  1.20556  1.20689  1.20442  1.20469\n",
       "4  1.20468  1.20599  1.20380  1.20573"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_num = 5\n",
    "sequence_length = 6  # Number of time steps to consider\n",
    "\n",
    "# Define file and directory names\n",
    "file_name = '0_61938.csv'\n",
    "data_dir = 'ohlc_data'\n",
    "# parent_dir = 'forexPredictor'\n",
    "# repo = 'Repos_git'\n",
    "# repo_dir = 'Make_Money_with_Tensorflow_2.0'\n",
    "# Get the absolute base directory dynamically\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Move up one level\n",
    "\n",
    "# Construct the full file path in an OS-independent way\n",
    "# data_path = os.path.join(base_dir, repo, repo_dir, parent_dir, data_dir, file_name)\n",
    "data_path = os.path.join(base_dir, data_dir, file_name)\n",
    "\n",
    "# Check if the file exists before using it\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"✅ File found: {data_path}\")\n",
    "else:\n",
    "    print(f\"❌ Error: File not found at {data_path}\")\n",
    "\n",
    "\n",
    "ucols=['Open', 'High', 'Low', 'Close']\n",
    "data = pd.read_csv(data_path, usecols=ucols)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d72b258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(df)\n",
    "\n",
    "# Reshape for LSTM input\n",
    "data = data.reshape((data.shape[0], data.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "39a5b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dwt_flat(data):\n",
    "    coeffs_level1 = [pywt.dwt(sample.flatten(), 'haar') for sample in data]  # First-level DWT\n",
    "    cA1, cD1 = zip(*coeffs_level1)  # Approximation and detail coefficients (2 features)\n",
    "\n",
    "    coeffs_level2 = [pywt.dwt(cA, 'haar') for cA in cA1]  # Second-level DWT on Approximation\n",
    "    cA2, cD2 = zip(*coeffs_level2)  # Now we have 4 features\n",
    "\n",
    "    transformed_data = np.array(list(zip(cA2, cD2, cD1)))  # Combine all 4 coefficients\n",
    "    return transformed_data.reshape(transformed_data.shape[0], transformed_data.shape[1], 1)  # Ensure shape (713, 4, 1)\n",
    "\n",
    "def apply_dwt(data):\n",
    "    coeffs = [pywt.dwt(sample, 'haar') for sample in data]\n",
    "    cA, cD = zip(*coeffs)  # Approximation and Detail coefficients\n",
    "    return np.array(cA)  # Using only approximation coefficients\n",
    "\n",
    "# Define Autoencoder with LSTM for better feature extraction\n",
    "def create_autoencoder(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = LSTM(64, return_sequences=True)(input_layer)\n",
    "    x = LSTM(32, return_sequences=False)(x)\n",
    "    encoded = Dense(16, activation='relu')(x)\n",
    "\n",
    "    decoded = Dense(32, activation='relu')(encoded)\n",
    "    decoded = Dense(input_shape[0], activation='sigmoid')(decoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder, encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8055ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6293dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to NumPy array\n",
    "data = df.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2fc839e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1936/1936 [==============================] - 9s 3ms/step - loss: 0.0089\n",
      "Epoch 2/5\n",
      "1936/1936 [==============================] - 6s 3ms/step - loss: 0.0059\n",
      "Epoch 3/5\n",
      "1936/1936 [==============================] - 6s 3ms/step - loss: 0.0059\n",
      "Epoch 4/5\n",
      "1936/1936 [==============================] - 6s 3ms/step - loss: 0.0059\n",
      "Epoch 5/5\n",
      "1936/1936 [==============================] - 6s 3ms/step - loss: 0.0059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2178e488fd0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply DWT\n",
    "transformed_data = apply_dwt(data)\n",
    "\n",
    "# Train Autoencoder\n",
    "autoencoder, encoder = create_autoencoder(transformed_data.shape[1:])\n",
    "autoencoder.fit(transformed_data, transformed_data, epochs=5, batch_size=32, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5305d019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "autoencoder.save(\"encoder_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7bb3994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1936/1936 [==============================] - 3s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Extract Features\n",
    "features = encoder.predict(transformed_data)\n",
    "\n",
    "# Apply TimeSeriesKMeans Clustering\n",
    "num_clusters = 3  # Buy (2), Sell (1), Hold (0)\n",
    "tskmeans = TimeSeriesKMeans(n_clusters=num_clusters, metric=\"dtw\", random_state=42)\n",
    "clusters = tskmeans.fit_predict(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88f9ac6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9910/9910 [==============================] - 9s 890us/step - loss: 0.1636 - accuracy: 0.9511 - val_loss: 0.0713 - val_accuracy: 0.9856\n",
      "Epoch 2/5\n",
      "9910/9910 [==============================] - 8s 841us/step - loss: 0.0562 - accuracy: 0.9902 - val_loss: 0.0454 - val_accuracy: 0.9907\n",
      "Epoch 3/5\n",
      "9910/9910 [==============================] - 8s 844us/step - loss: 0.0406 - accuracy: 0.9947 - val_loss: 0.0372 - val_accuracy: 0.9879\n",
      "Epoch 4/5\n",
      "9910/9910 [==============================] - 9s 873us/step - loss: 0.0334 - accuracy: 0.9956 - val_loss: 0.0310 - val_accuracy: 0.9975\n",
      "Epoch 5/5\n",
      "9910/9910 [==============================] - 8s 835us/step - loss: 0.0294 - accuracy: 0.9964 - val_loss: 0.0279 - val_accuracy: 0.9964\n",
      "388/388 [==============================] - 0s 661us/step - loss: 0.0279 - accuracy: 0.9964\n",
      "Test Accuracy: 99.64%\n"
     ]
    }
   ],
   "source": [
    "# Convert clusters to one-hot encoding\n",
    "clusters_one_hot = tf.keras.utils.to_categorical(clusters, num_clusters)\n",
    "\n",
    "# Split data into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, clusters_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure input shape consistency\n",
    "input_shape = features.shape[1:]  # Extract feature dimension\n",
    "classifier_input = Input(shape=input_shape)\n",
    "classifier_output = Dense(num_clusters, activation=\"softmax\")(classifier_input)\n",
    "\n",
    "# Create and compile classifier\n",
    "final_model = Model(classifier_input, classifier_output)\n",
    "final_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "epoch_num = 5  # Define the number of epochs\n",
    "final_model.fit(X_train, y_train, epochs=epoch_num, batch_size=5, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model on test data\n",
    "loss, accuracy = final_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83fdd44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "final_model.save(\"trading_model_dwt_autoEnc.h5\")\n",
    "loaded_model = load_model(\"trading_model_dwt_autoEnc.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ff088db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# documentation: https://www.mql5.com/en/docs/integration/python_metatrader5\n",
    "\n",
    "import MetaTrader5 as mt  # pip install MetaTrader5\n",
    "import pandas as pd  # pip install pandas\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# start the platform with initialize()\n",
    "mt.initialize()\n",
    "\n",
    "# login to Trade Account with login()\n",
    "# make sure that trade server is enabled in MT5 client terminal\n",
    "\n",
    "login = 165905041\n",
    "password = 'iIeElL0176_'\n",
    "server = 'XMGlobal-MT5 2'\n",
    "\n",
    "mt.login(login, password, server)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0428ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define symbol and timeframe\n",
    "symbol = \"EURUSD\"\n",
    "timeframe = mt.TIMEFRAME_H1\n",
    "\n",
    "# Get data for the last 48 hours\n",
    "end_time = datetime.now()\n",
    "start_time = end_time - timedelta(hours=1000)\n",
    "\n",
    "# Retrieve OHLC data\n",
    "ohlc_data = pd.DataFrame(mt.copy_rates_range(symbol, timeframe, start_time, end_time))\n",
    "\n",
    "# Convert time column to datetime format\n",
    "ohlc_data['time'] = pd.to_datetime(ohlc_data['time'], unit='s')\n",
    "\n",
    "\n",
    "ohlc_data.rename(columns={'open':'Open', 'high':'High', \n",
    "                        'low':'Low', 'close':'Close', 'tick_volume':'Volume'}, inplace=True)\n",
    "columns_to_drop = ['time', 'spread', 'real_volume', 'Volume']\n",
    "ohlc_data.drop(columns=[col for col in columns_to_drop if col in ohlc_data.columns], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b529b475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.03885</td>\n",
       "      <td>1.03925</td>\n",
       "      <td>1.03837</td>\n",
       "      <td>1.03902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.03902</td>\n",
       "      <td>1.03965</td>\n",
       "      <td>1.03871</td>\n",
       "      <td>1.03908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.03909</td>\n",
       "      <td>1.03955</td>\n",
       "      <td>1.03798</td>\n",
       "      <td>1.03811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.03809</td>\n",
       "      <td>1.03854</td>\n",
       "      <td>1.03789</td>\n",
       "      <td>1.03831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.03831</td>\n",
       "      <td>1.03913</td>\n",
       "      <td>1.03815</td>\n",
       "      <td>1.03913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>1.07844</td>\n",
       "      <td>1.07875</td>\n",
       "      <td>1.07812</td>\n",
       "      <td>1.07861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>1.07857</td>\n",
       "      <td>1.07906</td>\n",
       "      <td>1.07803</td>\n",
       "      <td>1.07851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>1.07851</td>\n",
       "      <td>1.07988</td>\n",
       "      <td>1.07843</td>\n",
       "      <td>1.07983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>1.07983</td>\n",
       "      <td>1.08019</td>\n",
       "      <td>1.07920</td>\n",
       "      <td>1.07949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>1.07948</td>\n",
       "      <td>1.08008</td>\n",
       "      <td>1.07856</td>\n",
       "      <td>1.07910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>713 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Open     High      Low    Close\n",
       "0    1.03885  1.03925  1.03837  1.03902\n",
       "1    1.03902  1.03965  1.03871  1.03908\n",
       "2    1.03909  1.03955  1.03798  1.03811\n",
       "3    1.03809  1.03854  1.03789  1.03831\n",
       "4    1.03831  1.03913  1.03815  1.03913\n",
       "..       ...      ...      ...      ...\n",
       "708  1.07844  1.07875  1.07812  1.07861\n",
       "709  1.07857  1.07906  1.07803  1.07851\n",
       "710  1.07851  1.07988  1.07843  1.07983\n",
       "711  1.07983  1.08019  1.07920  1.07949\n",
       "712  1.07948  1.08008  1.07856  1.07910\n",
       "\n",
       "[713 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohlc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "99c6c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_data = ohlc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d5c1601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the saved scaler (Assuming it's saved from training to ensure consistency)\n",
    "# import joblib\n",
    "# scaler = joblib.load(\"scaler.pkl\")\n",
    "\n",
    "# Load new data\n",
    "new_data = new_data[['Open', 'High', 'Low', 'Close']].values  # Extract OHLC values\n",
    "\n",
    "# Normalize using the same MinMaxScaler used during training\n",
    "ohlc_data_scaled = scaler.transform(new_data)  # Use transform, NOT fit_transform\n",
    "\n",
    "# Reshape for LSTM input\n",
    "data = ohlc_data_scaled.reshape((new_data.shape[0], new_data.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2e7fa6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(713, 4, 1)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7b9b0f89",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (713, 3) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Apply DWT\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m transformed_data \u001b[38;5;241m=\u001b[39m \u001b[43mapply_dwt_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformed Data Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, transformed_data\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[70], line 8\u001b[0m, in \u001b[0;36mapply_dwt_flat\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      5\u001b[0m coeffs_level2 \u001b[38;5;241m=\u001b[39m [pywt\u001b[38;5;241m.\u001b[39mdwt(cA, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhaar\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m cA \u001b[38;5;129;01min\u001b[39;00m cA1]  \u001b[38;5;66;03m# Second-level DWT on Approximation\u001b[39;00m\n\u001b[0;32m      6\u001b[0m cA2, cD2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mcoeffs_level2)  \u001b[38;5;66;03m# Now we have 4 features\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m transformed_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcA2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcD2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcD1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Combine all 4 coefficients\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transformed_data\u001b[38;5;241m.\u001b[39mreshape(transformed_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], transformed_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (713, 3) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Apply DWT\n",
    "transformed_data = apply_dwt_flat(data)\n",
    "print(\"Transformed Data Shape:\", transformed_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dc3a2dfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Teacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2416, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Teacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2401, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Teacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2389, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\Teacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2357, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\Teacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Teacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_10\" is incompatible with the layer: expected shape=(None, 4, 1), found shape=(None, 2, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m classifier \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrading_model_dwt_autoEnc.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Extract features using the trained encoder\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     12\u001b[0m predictions \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9ndyvsdr.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Teacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2416, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Teacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2401, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Teacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2389, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\Teacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2357, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\Teacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Teacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_10\" is incompatible with the layer: expected shape=(None, 4, 1), found shape=(None, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load trained encoder and classifier\n",
    "encoder = tf.keras.models.load_model(\"encoder_model.h5\")\n",
    "classifier = tf.keras.models.load_model(\"trading_model_dwt_autoEnc.h5\")\n",
    "\n",
    "# Extract features using the trained encoder\n",
    "features = encoder.predict(transformed_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = classifier.predict(features)\n",
    "predicted_classes = np.argmax(predictions, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "# Map predictions to labels\n",
    "labels = {0: \"Hold\", 1: \"Sell\", 2: \"Buy\"}\n",
    "predicted_labels = [labels[p] for p in predicted_classes]\n",
    "\n",
    "# Align predictions with the original DataFrame\n",
    "new_data_df = pd.DataFrame(new_data, columns=['Open', 'High', 'Low', 'Close'])\n",
    "new_data_df[\"Prediction\"] = predicted_labels  # Add predicted actions to DataFrame\n",
    "\n",
    "# Save the results\n",
    "new_data_df.to_csv(\"predicted_results.csv\", index=False)\n",
    "\n",
    "# Print first 10 rows with predictions\n",
    "print(new_data_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb3872f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
